# Next Frame Video Prediction with Convolutional LSTMs
 In this project, 5 walking animations, each consisting of approximately 250 frames and lasting about 10 seconds, created with Blender 3.5 animation software were used. Convolutional LSTMs (Convolutional LSTMs) architecture was utilized for processing and predicting the images. Each animation video in the dataset was converted to grayscale, resized to 64x64, and combined into moving npy files consisting of 24 frames. As a result, a single-channel dataset of 1250 sequences, each 24 frames long and 64x64 in size, was created. The predictions generated at the end of the study were presented as GIFs.
